python real_time_proxy_checker.py your_github_token_here
# ä»£ç†IPæ± ç®¡ç†ç³»ç»Ÿä½¿ç”¨æ–‡æ¡?

## ç›®å½•
- [é¡¹ç›®ä»‹ç»](#é¡¹ç›®ä»‹ç»)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€?
- [å®‰è£…æŒ‡å—](#å®‰è£…æŒ‡å—)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§?
- [æ ¸å¿ƒæ¨¡å—ä½¿ç”¨](#æ ¸å¿ƒæ¨¡å—ä½¿ç”¨)
- [Twitterçˆ¬è™«ä½¿ç”¨](#twitterçˆ¬è™«ä½¿ç”¨)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [è‡ªåŠ¨åŒ–éƒ¨ç½²](#è‡ªåŠ¨åŒ–éƒ¨ç½?
- [APIæ–‡æ¡£](#apiæ–‡æ¡£)
- [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## é¡¹ç›®ä»‹ç»

Parser Proxy Poll æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ä»£ç†IPæ± ç®¡ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä»å¤šä¸ªå…è´¹ä»£ç†ç½‘ç«™æ”¶é›†ä»£ç†IPï¼ŒéªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶æä¾›ç»™çˆ¬è™«ç¨‹åºä½¿ç”¨ã€‚é¡¹ç›®è¿˜åŒ…å«äº†Twitterç”¨æˆ·èµ„æ–™çˆ¬è™«åŠŸèƒ½ã€?

### ä¸»è¦åŠŸèƒ½
- ğŸ” å¤šæºä»£ç†çˆ¬å–
- âœ?é«˜å¹¶å‘ä»£ç†éªŒè¯?
- â˜ï¸ GitHubäº‘ç«¯å­˜å‚¨
- ğŸ¤– Twitteræ•°æ®é‡‡é›†
- âš?å®æ—¶çŠ¶æ€ç›‘æ?
- ğŸ”„ è‡ªåŠ¨åŒ–è¿è¡?

## åŠŸèƒ½ç‰¹æ€?

### 1. ä»£ç†æºæ”¯æŒ?
- ç±³æ‰‘ä»£ç†
- ä»£ç†66
- å¼€å¿ƒä»£ç?
- è¶é¸ŸIP
- å¿«ä»£ç?
- PROXY11

### 2. éªŒè¯æœºåˆ¶
- å¤šçº¿ç¨‹å¹¶å‘éªŒè¯?
- æ™ºèƒ½è¶…æ—¶æ§åˆ¶
- å®æ—¶è¿›åº¦æ˜¾ç¤º
- å¤±è´¥è‡ªåŠ¨é‡è¯•

### 3. å­˜å‚¨æ–¹æ¡ˆ
- æœ¬åœ°æ–‡ä»¶å­˜å‚¨
- GitHubä»“åº“å­˜å‚¨
- è‡ªåŠ¨å»é‡å¤„ç†

## å®‰è£…æŒ‡å—

### ç¯å¢ƒè¦æ±‚
- Python 3.6+
- Git
- Chromeæµè§ˆå™¨ï¼ˆå¦‚æœä½¿ç”¨Seleniumçˆ¬è™«ï¼?

### 1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/your-repo/parser_proxy.git
cd parser_proxy
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. å®‰è£…ä¾èµ–
```bash
cd parser_proxy_1
pip install -r requirements.txt
```

### 4. å®‰è£…é¢å¤–ä¾èµ–ï¼ˆTwitterçˆ¬è™«ï¼?
```bash
# å¦‚æœä½¿ç”¨Seleniumç‰ˆæœ¬
pip install selenium pandas

# ä¸‹è½½ChromeDriver
# è®¿é—® https://chromedriver.chromium.org/
# ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„ChromeDriverå¹¶é…ç½®åˆ°PATH
```

## å¿«é€Ÿå¼€å§?

### 1. è·å–ä»£ç†åˆ—è¡¨

**å‘½ä»¤è¡Œæ‰§è¡Œï¼š**
```bash
cd parser_proxy_1
python -c "
import proxyFetcher
proxies = []
for proxy in proxyFetcher.freeProxy01():
    proxies.append(proxy)
    print(proxy)
print(f'å…±è·å?{len(proxies)} ä¸ªä»£ç?)
"
```

**Pythonè„šæœ¬ï¼?*
```python
from proxyFetcher import *

# è·å–æ‰€æœ‰ä»£ç?
all_proxies = []

# ä»å„ä¸ªä»£ç†æºè·å–
for proxy in freeProxy01():
    all_proxies.append(proxy)
    
for proxy in freeProxy02():
    all_proxies.append(proxy)
    
# ... å…¶ä»–ä»£ç†æº?

print(f"å…±è·å?{len(all_proxies)} ä¸ªä»£ç?)
```

### 2. éªŒè¯ä»£ç†

**å‘½ä»¤è¡Œæ‰§è¡Œï¼š**
```bash
cd parser_proxy_1
python -c "
from check_proxy import check_proxy
proxy = '127.0.0.1:8080'
if check_proxy(proxy):
    print(f'{proxy} æœ‰æ•ˆ')
else:
    print(f'{proxy} æ— æ•ˆ')
"
```

**Pythonè„šæœ¬ï¼?*
```python
from check_proxy import check_proxy

# éªŒè¯å•ä¸ªä»£ç†
proxy = "127.0.0.1:8080"
if check_proxy(proxy):
    print(f"{proxy} æœ‰æ•ˆ")
else:
    print(f"{proxy} æ— æ•ˆ")
```

### 3. ä½¿ç”¨å®æ—¶ä»£ç†æ£€æŸ¥å™¨

**å‘½ä»¤è¡Œæ‰§è¡Œï¼š**
```bash
cd parser_proxy_1
export GITHUB_TOKEN="your_github_token_here"
python real_time_proxy_checker.py
```

**Pythonè„šæœ¬ï¼?*
```python
from real_time_proxy_checker import RealTimeProxyChecker

# åˆå§‹åŒ–æ£€æŸ¥å™¨
checker = RealTimeProxyChecker(token="your_github_token")

# å¼€å§‹æ£€æŸ?
checker.start_checking()
```

### 4. ä¸€é”®è·å–å¹¶éªŒè¯ä»£ç†

**åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼š**
```bash
# åˆ›å»º quick_start.py
cat > quick_start.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
sys.path.append('parser_proxy_1')

from proxyFetcher import *
from check_proxy import check_proxy
import concurrent.futures

def get_all_proxies():
    """è·å–æ‰€æœ‰ä»£ç†æºçš„ä»£ç?""
    all_proxies = []
    
    proxy_functions = [
        freeProxy01, freeProxy02, freeProxy03, 
        freeProxy04, freeProxy05, freeProxy06
    ]
    
    for func in proxy_functions:
        try:
            print(f"æ­£åœ¨è·å– {func.__name__} çš„ä»£ç?..")
            proxies = list(func())
            all_proxies.extend(proxies)
            print(f"ä»?{func.__name__} è·å–åˆ?{len(proxies)} ä¸ªä»£ç?)
        except Exception as e:
            print(f"è·å– {func.__name__} å¤±è´¥: {e}")
    
    # å»é‡
    unique_proxies = list(set(all_proxies))
    print(f"\næ€»è®¡è·å– {len(all_proxies)} ä¸ªä»£ç†ï¼Œå»é‡å?{len(unique_proxies)} ä¸?)
    return unique_proxies

def validate_proxies(proxies, max_workers=50):
    """å¹¶å‘éªŒè¯ä»£ç†"""
    valid_proxies = []
    
    print(f"\nå¼€å§‹éªŒè¯?{len(proxies)} ä¸ªä»£ç?..")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {executor.submit(check_proxy, proxy): proxy for proxy in proxies}
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_proxy)):
            proxy = future_to_proxy[future]
            try:
                if future.result():
                    valid_proxies.append(proxy)
                    print(f"âœ?[{i+1}/{len(proxies)}] {proxy} æœ‰æ•ˆ")
                else:
                    print(f"âœ?[{i+1}/{len(proxies)}] {proxy} æ— æ•ˆ")
            except Exception as e:
                print(f"âœ?[{i+1}/{len(proxies)}] {proxy} é”™è¯¯: {e}")
    
    return valid_proxies

def save_proxies(proxies, filename="valid_proxies.txt"):
    """ä¿å­˜æœ‰æ•ˆä»£ç†åˆ°æ–‡ä»?""
    with open(filename, 'w') as f:
        for proxy in proxies:
            f.write(proxy + '\n')
    print(f"\næœ‰æ•ˆä»£ç†å·²ä¿å­˜åˆ° {filename}")

if __name__ == "__main__":
    # 1. è·å–æ‰€æœ‰ä»£ç?
    all_proxies = get_all_proxies()
    
    if not all_proxies:
        print("æœªè·å–åˆ°ä»»ä½•ä»£ç†ï¼Œç¨‹åºé€€å‡?)
        sys.exit(1)
    
    # 2. éªŒè¯ä»£ç†
    valid_proxies = validate_proxies(all_proxies)
    
    # 3. ä¿å­˜ç»“æœ
    if valid_proxies:
        save_proxies(valid_proxies)
        print(f"\næˆåŠŸéªŒè¯ {len(valid_proxies)} ä¸ªæœ‰æ•ˆä»£ç?)
        print(f"æˆåŠŸç? {len(valid_proxies)/len(all_proxies)*100:.2f}%")
    else:
        print("\næœªæ‰¾åˆ°æœ‰æ•ˆä»£ç?)
EOF

# è¿è¡Œè„šæœ¬
python quick_start.py
```

## æ ¸å¿ƒæ¨¡å—ä½¿ç”¨

### 1. ProxyFetcher - ä»£ç†è·å–å™?

```python
import proxyFetcher

# æ–¹æ³•1ï¼šè·å–ç±³æ‰‘ä»£ç?
for proxy in proxyFetcher.freeProxy01():
    print(proxy)

# æ–¹æ³•2ï¼šè·å–ä»£ç?6
for proxy in proxyFetcher.freeProxy02():
    print(proxy)

# æ–¹æ³•3ï¼šè·å–å¿«ä»£ç†ï¼ˆå¯æŒ‡å®šé¡µæ•°ï¼?
for proxy in proxyFetcher.freeProxy05(page_count=5):
    print(proxy)

# ä¿å­˜ä»£ç†åˆ°æ–‡ä»?
def save_all_proxies():
    proxies = []
    
    # æ”¶é›†æ‰€æœ‰ä»£ç?
    for func in [freeProxy01, freeProxy02, freeProxy03, 
                 freeProxy04, freeProxy05, freeProxy06]:
        try:
            for proxy in func():
                proxies.append(proxy)
        except Exception as e:
            print(f"è·å–ä»£ç†å¤±è´¥: {e}")
    
    # å»é‡å¹¶ä¿å­?
    proxies = list(set(proxies))
    with open("proxyData.txt", "w") as f:
        for proxy in proxies:
            f.write(proxy + "\n")
    
    return proxies
```

### 2. CheckProxy - ä»£ç†éªŒè¯å™?

```python
from check_proxy import check_proxy
import concurrent.futures

# æ‰¹é‡éªŒè¯ä»£ç†
def batch_check_proxies(proxy_list, max_workers=50):
    valid_proxies = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ?
        future_to_proxy = {
            executor.submit(check_proxy, proxy): proxy 
            for proxy in proxy_list
        }
        
        # è·å–ç»“æœ
        for future in concurrent.futures.as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                if future.result():
                    valid_proxies.append(proxy)
                    print(f"âœ?{proxy}")
                else:
                    print(f"âœ?{proxy}")
            except Exception as e:
                print(f"âœ?{proxy} - é”™è¯¯: {e}")
    
    return valid_proxies

# ä½¿ç”¨ç¤ºä¾‹
proxies = ["1.2.3.4:8080", "5.6.7.8:3128"]
valid = batch_check_proxies(proxies)
print(f"æœ‰æ•ˆä»£ç†: {len(valid)}/{len(proxies)}")
```

### 3. GitHub API - äº‘ç«¯å­˜å‚¨

```python
import github_api
import os

# è®¾ç½®GitHub Token
token = os.getenv('GITHUB_TOKEN', "your_token_here")

# è¯»å–GitHubä¸Šçš„ä»£ç†åˆ—è¡¨
def get_github_proxies():
    try:
        content = github_api.get_content(
            "parserpp",           # ç”¨æˆ·å?
            "ip_ports",          # ä»“åº“å?
            "/proxyinfo.txt",    # æ–‡ä»¶è·¯å¾„
            token
        )
        return content.split("\n")
    except Exception as e:
        print(f"è·å–å¤±è´¥: {e}")
        return []

# æ›´æ–°GitHubä¸Šçš„ä»£ç†åˆ—è¡¨
def update_github_proxies(proxy_list):
    content = "\n".join(proxy_list)
    
    try:
        result = github_api.update_content(
            "parserpp",
            "ip_ports", 
            "/proxyinfo.txt",
            _token=token,
            _content_not_base64=content,
            _commit_msg="æ›´æ–°ä»£ç†åˆ—è¡¨"
        )
        print("æ›´æ–°æˆåŠŸ")
    except Exception as e:
        print(f"æ›´æ–°å¤±è´¥: {e}")

# åˆ›å»ºæ–°çš„ä»£ç†æ–‡ä»¶
def create_proxy_file(filename, proxy_list):
    content = "\n".join(proxy_list)
    
    result = github_api.create_file(
        "parserpp",
        "ip_ports",
        f"/{filename}",
        _token=token,
        _content_not_base64=content,
        _commit_msg=f"åˆ›å»º{filename}"
    )
```

### 4. RealTimeProxyChecker - å®æ—¶æ£€æŸ¥å™¨

```python
from real_time_proxy_checker import RealTimeProxyChecker

class MyProxyChecker(RealTimeProxyChecker):
    def __init__(self):
        super().__init__(token="your_github_token")
        
    def run(self):
        # è·å–ä»£ç†åˆ—è¡¨
        proxy_list = self.get_proxy_list()
        
        # è®¾ç½®å¹¶å‘æ•?
        self.check_proxies_concurrent(proxy_list, max_workers=100)
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\næ€»è®¡: {self.total_count}")
        print(f"æœ‰æ•ˆ: {len(self.valid_proxies)}")
        print(f"æˆåŠŸç? {len(self.valid_proxies)/self.total_count*100:.2f}%")

# ä½¿ç”¨
checker = MyProxyChecker()
checker.run()
```

## Twitterçˆ¬è™«ä½¿ç”¨

### 1. åŸºç¡€ç‰ˆçˆ¬è™«ï¼ˆrequestsï¼?

**å‘½ä»¤è¡Œæ‰§è¡Œï¼š**
```bash
# ç›´æ¥è¿è¡ŒTwitterçˆ¬è™«
cd parser_proxy_1
python -c "
from twitter_profile_scraper import TwitterProfileScraper
scraper = TwitterProfileScraper()
users = ['elonmusk', 'BillGates', 'tim_cook']
scraper.scrape_profiles(users, max_workers=3)
"
```

**åˆ›å»ºTwitterçˆ¬è™«è„šæœ¬ï¼?*
```bash
# åˆ›å»º twitter_scraper.py
cat > twitter_scraper.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
sys.path.append('.')

from twitter_profile_scraper import TwitterProfileScraper
import pandas as pd

def main():
    # åˆå§‹åŒ–çˆ¬è™?
    scraper = TwitterProfileScraper(
        proxy_file='parser_proxy_1/valid_proxies.txt',
        csv_file='twitter_data.csv'
    )
    
    # è®¾ç½®ç”¨æˆ·åˆ—è¡¨
    users = ['elonmusk', 'BillGates', 'tim_cook', 'sundarpichai', 'jeffbezos']
    
    print(f"å¼€å§‹çˆ¬å?{len(users)} ä¸ªç”¨æˆ·çš„èµ„æ–™...")
    
    # å¼€å§‹çˆ¬å?
    scraper.scrape_profiles(users, max_workers=3)
    
    # æ˜¾ç¤ºç»“æœ
    try:
        df = pd.read_csv('twitter_data.csv')
        print(f"\nçˆ¬å–å®Œæˆï¼å…±è·å– {len(df)} ä¸ªç”¨æˆ·èµ„æ–?)
        print("\nå‰?ä¸ªç”¨æˆ?")
        print(df[['username', 'followers', 'has_contact']].head())
    except Exception as e:
        print(f"è¯»å–ç»“æœå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
EOF

# è¿è¡Œè„šæœ¬
python twitter_scraper.py
```

**Pythonè„šæœ¬ï¼?*
```python
from twitter_profile_scraper import TwitterProfileScraper

# åˆå§‹åŒ–çˆ¬è™?
scraper = TwitterProfileScraper(
    proxy_file='parser_proxy_1/valid_proxies.txt',
    csv_file='twitter_data.csv'
)

# è®¾ç½®ç”¨æˆ·åˆ—è¡¨
users = ['elonmusk', 'BillGates', 'tim_cook']

# å¼€å§‹çˆ¬å?
scraper.scrape_profiles(users, max_workers=5)

# è‡ªå®šä¹‰ç­›é€‰æ¡ä»?
def custom_filter(bio):
    # åªä¿ç•™åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç”¨æˆ?
    keywords = ['CEO', 'Founder', 'Engineer']
    return any(keyword in bio for keyword in keywords)

scraper.bio_filter = custom_filter
```

### 2. Seleniumç‰ˆçˆ¬è™«ï¼ˆé«˜çº§ï¼?

**å®‰è£…ChromeDriverï¼?*
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y wget unzip
wget -O /tmp/chromedriver.zip https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip
unzip /tmp/chromedriver.zip -d /tmp/
sudo mv /tmp/chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver

# Windows (ä½¿ç”¨PowerShell)
# ä¸‹è½½å¹¶è§£å‹åˆ° C:\chromedriver\
# æ·»åŠ  C:\chromedriver\ åˆ°ç³»ç»ŸPATHç¯å¢ƒå˜é‡

# macOS
brew install chromedriver
```

**å‘½ä»¤è¡Œæ‰§è¡Œï¼š**
```bash
# è¿è¡ŒSeleniumç‰ˆTwitterçˆ¬è™«
python -c "
from twitter_profile_scraper_selenium import TwitterProfileScraperSelenium
scraper = TwitterProfileScraperSelenium()
users = ['nasa', 'Google', 'Microsoft']
scraper.scrape_with_selenium(users, max_workers=2)
"
```

**åˆ›å»ºSeleniumçˆ¬è™«è„šæœ¬ï¼?*
```bash
# åˆ›å»º twitter_selenium_scraper.py
cat > twitter_selenium_scraper.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
sys.path.append('.')

from twitter_profile_scraper_selenium import TwitterProfileScraperSelenium
from selenium.webdriver.chrome.options import Options
import pandas as pd

def get_chrome_options(proxy=None):
    """é…ç½®Chromeé€‰é¡¹"""
    options = Options()
    options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    
    if proxy:
        options.add_argument(f'--proxy-server=http://{proxy}')
    
    return options

def main():
    # åˆå§‹åŒ–Seleniumçˆ¬è™«
    scraper = TwitterProfileScraperSelenium(
        proxy_file='parser_proxy_1/valid_proxies.txt',
        csv_file='twitter_selenium_data.csv'
    )
    
    # ç”¨æˆ·åˆ—è¡¨
    users = [
        'nasa', 'Google', 'Microsoft', 'Apple', 'Tesla',
        'SpaceX', 'OpenAI', 'nvidia', 'AMD', 'Intel'
    ]
    
    print(f"ä½¿ç”¨Seleniumçˆ¬å– {len(users)} ä¸ªç”¨æˆ?..")
    print("æ³¨æ„ï¼šSeleniumæ¨¡å¼è¾ƒæ…¢ä½†æ›´ç¨³å®š")
    
    try:
        # å¼€å§‹çˆ¬å–ï¼ˆé™ä½å¹¶å‘æ•°é¿å…è¢«å°ï¼‰
        scraper.scrape_with_selenium(
            user_list=users,
            max_workers=2,  # å¹¶å‘æ•?
            retry_times=3   # é‡è¯•æ¬¡æ•°
        )
        
        # æ˜¾ç¤ºç»“æœ
        df = pd.read_csv('twitter_selenium_data.csv')
        print(f"\nçˆ¬å–å®Œæˆï¼å…±è·å– {len(df)} ä¸ªç”¨æˆ·èµ„æ–?)
        
        # ç»Ÿè®¡ä¿¡æ¯
        contact_users = df[df['has_contact'] == True]
        print(f"æœ‰è”ç³»æ–¹å¼çš„ç”¨æˆ·: {len(contact_users)}")
        print(f"å¹³å‡ç²‰ä¸æ•? {df['followers'].mean():.0f}")
        
    except Exception as e:
        print(f"çˆ¬å–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
EOF

# è¿è¡Œè„šæœ¬
python twitter_selenium_scraper.py
```

**Pythonè„šæœ¬ï¼?*
```python
from twitter_profile_scraper_selenium import TwitterProfileScraperSelenium

# åˆå§‹åŒ?
scraper = TwitterProfileScraperSelenium()

# é…ç½®Chromeé€‰é¡¹
def get_chrome_options(proxy=None):
    options = Options()
    options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    if proxy:
        options.add_argument(f'--proxy-server=http://{proxy}')
    
    return options

# æ‰¹é‡çˆ¬å–
scraper.scrape_with_selenium(
    user_list=['nasa', 'Google', 'Microsoft'],
    max_workers=3,  # é™ä½å¹¶å‘æ•°ï¼Œé¿å…è¢«å°
    retry_times=3
)
```

### 3. æ•°æ®å¤„ç†å’Œåˆ†æ?

```python
import pandas as pd

# è¯»å–çˆ¬å–ç»“æœ
df = pd.read_csv('twitter_data.csv')

# ç­›é€‰å•†åŠ¡åˆä½œç”¨æˆ?
business_users = df[df['has_contact'] == True]

# æŒ‰ç²‰ä¸æ•°æ’åº
top_users = df.sort_values('followers', ascending=False).head(100)

# å¯¼å‡ºè”ç³»æ–¹å¼
contacts = df[df['contact_info'].notna()][['username', 'contact_info']]
contacts.to_csv('contacts.csv', index=False)

# ç»Ÿè®¡åˆ†æ
print(f"æ€»ç”¨æˆ·æ•°: {len(df)}")
print(f"æœ‰è”ç³»æ–¹å¼? {df['has_contact'].sum()}")
print(f"å¹³å‡ç²‰ä¸æ•? {df['followers'].mean():.0f}")
```

## é…ç½®è¯´æ˜

### 1. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼?
```bash
# GitHub Token
GITHUB_TOKEN=your_github_token_here

# ä»£ç†è®¾ç½®
PROXY_TIMEOUT=3
MAX_WORKERS=50
CHECK_URL=http://icanhazip.com/

# Twitterçˆ¬è™«è®¾ç½®
TWITTER_MAX_RETRIES=3
TWITTER_DELAY_MIN=1
TWITTER_DELAY_MAX=3
```

### 2. ä»£ç†æºé…ç½?

ç¼–è¾‘ `proxyFetcher.py` æ·»åŠ æ–°çš„ä»£ç†æºï¼š
```python
def freeProxyCustom():
    """è‡ªå®šä¹‰ä»£ç†æº"""
    url = "https://your-proxy-source.com/api"
    
    resp = WebRequest().get(url).json
    for proxy in resp['data']:
        yield f"{proxy['ip']}:{proxy['port']}"
```

### 3. å®šæ—¶ä»»åŠ¡é…ç½®

ä½¿ç”¨ APScheduler è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼?
```python
from apscheduler.schedulers.blocking import BlockingScheduler
import subprocess

scheduler = BlockingScheduler()

# æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ä»£ç†æ›´æ–?
@scheduler.scheduled_job('interval', hours=1)
def update_proxy_pool():
    subprocess.run(['python', 'real_time_proxy_checker.py'])

# æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡?
@scheduler.scheduled_job('cron', hour=2)
def daily_cleanup():
    # æ¸…ç†æ— æ•ˆä»£ç†
    pass

scheduler.start()
```

## è‡ªåŠ¨åŒ–éƒ¨ç½?

### 1. GitHub Actions é…ç½®

**åˆ›å»ºGitHub Actionså·¥ä½œæµï¼š**
```bash
# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p .github/workflows

# åˆ›å»ºå·¥ä½œæµæ–‡ä»?
cat > .github/workflows/proxy-update.yml << 'EOF'
name: Update Proxy Pool

on:
  schedule:
    - cron: '0 */2 * * *'  # æ¯?å°æ—¶æ‰§è¡Œ
  workflow_dispatch:  # æ‰‹åŠ¨è§¦å‘

jobs:
  update:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        cd parser_proxy_1
        pip install -r requirements.txt
    
    - name: Run proxy checker
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        cd parser_proxy_1
        python real_time_proxy_checker.py
    
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        git diff --staged --quiet || git commit -m "è‡ªåŠ¨æ›´æ–°ä»£ç†æ±?$(date '+%Y-%m-%d %H:%M:%S')"
        git push
EOF

echo "GitHub Actionså·¥ä½œæµå·²åˆ›å»º"
```

**è®¾ç½®GitHub Secretsï¼?*
```bash
# ä½¿ç”¨GitHub CLIè®¾ç½®å¯†é’¥
gh secret set GITHUB_TOKEN --body "your_github_token_here"

# æˆ–è€…æ‰‹åŠ¨åœ¨GitHubç½‘é¡µè®¾ç½®ï¼?
# 1. è¿›å…¥ä»“åº“ Settings > Secrets and variables > Actions
# 2. ç‚¹å‡» "New repository secret"
# 3. Name: GITHUB_TOKEN
# 4. Value: ä½ çš„GitHub Token
```

**æ‰‹åŠ¨è§¦å‘å·¥ä½œæµï¼š**
```bash
# ä½¿ç”¨GitHub CLI
gh workflow run proxy-update.yml

# æˆ–è€…åœ¨GitHubç½‘é¡µï¼?
# è¿›å…¥ Actions > Update Proxy Pool > Run workflow
```

### 2. Docker éƒ¨ç½²

**åˆ›å»ºDockerfileï¼?*
```bash
# åˆ›å»º Dockerfile
cat > Dockerfile << 'EOF'
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    cron \
    wget \
    unzip \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY parser_proxy_1/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# åˆ›å»ºå®šæ—¶ä»»åŠ¡
RUN echo "0 */2 * * * cd /app && python parser_proxy_1/real_time_proxy_checker.py" > /etc/cron.d/proxy-cron
RUN chmod 0644 /etc/cron.d/proxy-cron
RUN crontab /etc/cron.d/proxy-cron

# åˆ›å»ºå¯åŠ¨è„šæœ¬
RUN echo '#!/bin/bash\nservice cron start\ntail -f /var/log/cron.log' > /app/start.sh
RUN chmod +x /app/start.sh

CMD ["/app/start.sh"]
EOF

echo "Dockerfileå·²åˆ›å»?
```

**åˆ›å»ºdocker-compose.ymlï¼?*
```bash
# åˆ›å»º docker-compose.yml
cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  proxy-pool:
    build: .
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - TZ=Asia/Shanghai
    volumes:
      - ./data:/app/data
      - ./logs:/var/log
    restart: unless-stopped
    ports:
      - "8080:8080"  # å¦‚æœæœ‰Webç•Œé¢
    
  # å¯é€‰ï¼šæ·»åŠ Redisç¼“å­˜
  redis:
    image: redis:6-alpine
    restart: unless-stopped
    volumes:
      - redis-data:/data
    
  # å¯é€‰ï¼šæ·»åŠ æ•°æ®åº?
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_DB=proxy_db
      - POSTGRES_USER=proxy_user
      - POSTGRES_PASSWORD=proxy_pass
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  redis-data:
  postgres-data:
EOF

echo "docker-compose.ymlå·²åˆ›å»?
```

**æ„å»ºå’Œè¿è¡Œï¼š**
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export GITHUB_TOKEN="your_github_token_here"

# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f proxy-pool

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart

# è¿›å…¥å®¹å™¨è°ƒè¯•
docker-compose exec proxy-pool bash
```

**ç›´æ¥ä½¿ç”¨Dockerè¿è¡Œï¼?*
```bash
# æ„å»ºé•œåƒ
docker build -t proxy-pool .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name proxy-pool \
  -e GITHUB_TOKEN="your_github_token_here" \
  -v $(pwd)/data:/app/data \
  --restart unless-stopped \
  proxy-pool

# æŸ¥çœ‹æ—¥å¿—
docker logs -f proxy-pool

# åœæ­¢å®¹å™¨
docker stop proxy-pool

# åˆ é™¤å®¹å™¨
docker rm proxy-pool
```

## APIæ–‡æ¡£

### WebRequest ç±?

```python
from webRequest import WebRequest

# GETè¯·æ±‚
wr = WebRequest()
response = wr.get(
    url="https://api.example.com",
    header={'Authorization': 'Bearer token'},
    retry_time=3,
    retry_interval=5,
    timeout=10
)

# è·å–å“åº”
html_tree = response.tree  # lxmlæ ‘å¯¹è±?
text = response.text       # æ–‡æœ¬å†…å®¹
json_data = response.json  # JSONæ•°æ®
```

### ä»£ç†éªŒè¯ API

```python
from check_proxy import req, check_proxy

# æ–¹æ³•1ï¼šç›´æ¥éªŒè¯?
is_valid = check_proxy("1.2.3.4:8080")

# æ–¹æ³•2ï¼šè‡ªå®šä¹‰éªŒè¯
def custom_check(proxy, test_url="https://www.google.com"):
    return req(test_url, proxy)

# æ–¹æ³•3ï¼šæ‰¹é‡éªŒè¯å™¨ç±?
class ProxyValidator:
    def __init__(self, test_urls=None):
        self.test_urls = test_urls or ["http://icanhazip.com/"]
        
    def validate(self, proxy):
        for url in self.test_urls:
            if req(url, proxy):
                return True
        return False
```

## æ³¨æ„äº‹é¡¹

### 1. ä»£ç†è´¨é‡
- å…è´¹ä»£ç†ç¨³å®šæ€§å·®ï¼ŒæˆåŠŸç‡é€šå¸¸åœ?-20%
- å»ºè®®é…åˆä»˜è´¹ä»£ç†æœåŠ¡ä½¿ç”¨
- å®šæœŸæ›´æ–°ä»£ç†æ± ï¼Œç§»é™¤å¤±æ•ˆä»£ç†

### 2. çˆ¬è™«ç­–ç•¥
- æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è¢«å°IP
- ä½¿ç”¨éšæœºUser-Agent
- è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—?
- å®ç°æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½

### 3. æ•°æ®å­˜å‚¨
- GitHub APIæœ‰é€Ÿç‡é™åˆ¶ï¼?000æ¬?å°æ—¶ï¼?
- å¤§é‡æ•°æ®å»ºè®®ä½¿ç”¨æ•°æ®åº“å­˜å‚?
- å®šæœŸå¤‡ä»½é‡è¦æ•°æ®

### 4. æ³•å¾‹åˆè§„
- éµå®ˆç½‘ç«™çš„robots.txtè§„åˆ™
- ä¸è¦çˆ¬å–ä¸ªäººéšç§ä¿¡æ¯
- åˆç†ä½¿ç”¨çˆ¬å–çš„æ•°æ?
- æ³¨æ„ç‰ˆæƒå’Œä½¿ç”¨æ¡æ¬?

## æ•…éšœæ’é™¤

### 1. å¸¸è§é”™è¯¯

**é”™è¯¯ï¼š`requests.exceptions.ProxyError`**
```python
# è§£å†³æ–¹æ¡ˆï¼šå¢åŠ è¶…æ—¶é‡è¯?
try:
    response = requests.get(url, proxies=proxy, timeout=5)
except requests.exceptions.ProxyError:
    # æ ‡è®°ä»£ç†æ— æ•ˆï¼Œä½¿ç”¨ä¸‹ä¸€ä¸?
    pass
```

**é”™è¯¯ï¼š`GitHub API rate limit exceeded`**
```python
# è§£å†³æ–¹æ¡ˆï¼šæ·»åŠ å»¶æ—?
import time

def github_request_with_delay(func, *args, **kwargs):
    result = func(*args, **kwargs)
    time.sleep(1)  # å»¶æ—¶1ç§?
    return result
```

**é”™è¯¯ï¼š`selenium.common.exceptions.WebDriverException`**
```python
# è§£å†³æ–¹æ¡ˆï¼šæ­£ç¡®é…ç½®ChromeDriver
# 1. ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„ChromeDriver
# 2. æ·»åŠ åˆ°ç³»ç»ŸPATH
# 3. æˆ–æŒ‡å®šè·¯å¾„ï¼š
driver = webdriver.Chrome(executable_path='/path/to/chromedriver')
```

### 2. æ€§èƒ½ä¼˜åŒ–

**æé«˜ä»£ç†éªŒè¯é€Ÿåº¦**
```python
# ä½¿ç”¨asyncioå¼‚æ­¥éªŒè¯
import asyncio
import aiohttp

async def async_check_proxy(session, proxy):
    try:
        async with session.get(
            'http://icanhazip.com',
            proxy=f'http://{proxy}',
            timeout=3
        ) as response:
            return response.status == 200
    except:
        return False

async def batch_check_async(proxy_list):
    async with aiohttp.ClientSession() as session:
        tasks = [async_check_proxy(session, proxy) for proxy in proxy_list]
        results = await asyncio.gather(*tasks)
    return results
```

**å‡å°‘å†…å­˜ä½¿ç”¨**
```python
# ä½¿ç”¨ç”Ÿæˆå™¨è€Œä¸æ˜¯åˆ—è¡?
def get_proxies_generator():
    for func in [freeProxy01, freeProxy02, ...]:
        yield from func()

# åˆ†æ‰¹å¤„ç†
def process_in_batches(proxies, batch_size=100):
    batch = []
    for proxy in proxies:
        batch.append(proxy)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch
```

### 3. è°ƒè¯•æŠ€å·?

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# ä¿å­˜è°ƒè¯•ä¿¡æ¯
def debug_proxy_check(proxy):
    logger = logging.getLogger(__name__)
    
    logger.debug(f"å¼€å§‹æ£€æŸ¥ä»£ç? {proxy}")
    start_time = time.time()
    
    result = check_proxy(proxy)
    
    elapsed = time.time() - start_time
    logger.debug(f"ä»£ç† {proxy} æ£€æŸ¥å®Œæˆ? {result}, è€—æ—¶: {elapsed:.2f}ç§?)
    
    return result
```

## æ‰©å±•å¼€å?

### 1. æ·»åŠ æ–°çš„ä»£ç†æº?

```python
# åœ?proxyFetcher.py ä¸­æ·»åŠ?
def freeProxyNew():
    """æ–°ä»£ç†æºç¤ºä¾‹"""
    api_url = "https://new-proxy-api.com/get"
    
    headers = {
        'User-Agent': 'Mozilla/5.0',
        'Referer': 'https://new-proxy-api.com'
    }
    
    try:
        response = requests.get(api_url, headers=headers)
        data = response.json()
        
        for item in data['proxies']:
            # æ ¹æ®APIè¿”å›æ ¼å¼è§£æ
            ip = item['ip']
            port = item['port']
            yield f"{ip}:{port}"
            
    except Exception as e:
        print(f"è·å–æ–°ä»£ç†æºå¤±è´¥: {e}")
```

### 2. è‡ªå®šä¹‰éªŒè¯é€»è¾‘

```python
class CustomProxyChecker:
    def __init__(self, test_sites=None):
        self.test_sites = test_sites or [
            'http://httpbin.org/ip',
            'https://api.ipify.org',
            'http://icanhazip.com'
        ]
    
    def check(self, proxy):
        """å¤šç«™ç‚¹éªŒè¯?""
        success_count = 0
        
        for site in self.test_sites:
            try:
                resp = requests.get(
                    site,
                    proxies={'http': proxy, 'https': proxy},
                    timeout=5
                )
                if resp.status_code == 200:
                    success_count += 1
            except:
                pass
        
        # è‡³å°‘2ä¸ªç«™ç‚¹æˆåŠŸæ‰ç®—æœ‰æ•?
        return success_count >= 2
```

### 3. é›†æˆæ•°æ®åº“å­˜å‚?

```python
import sqlite3
from datetime import datetime

class ProxyDatabase:
    def __init__(self, db_path='proxies.db'):
        self.conn = sqlite3.connect(db_path)
        self.init_db()
    
    def init_db(self):
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS proxies (
                id INTEGER PRIMARY KEY,
                ip_port TEXT UNIQUE,
                last_check TIMESTAMP,
                success_count INTEGER DEFAULT 0,
                fail_count INTEGER DEFAULT 0,
                status TEXT DEFAULT 'active'
            )
        ''')
        self.conn.commit()
    
    def add_proxy(self, ip_port):
        try:
            self.conn.execute(
                'INSERT INTO proxies (ip_port, last_check) VALUES (?, ?)',
                (ip_port, datetime.now())
            )
            self.conn.commit()
        except sqlite3.IntegrityError:
            pass  # å·²å­˜åœ?
    
    def update_proxy_status(self, ip_port, success):
        if success:
            self.conn.execute(
                '''UPDATE proxies 
                   SET success_count = success_count + 1, 
                       last_check = ?, 
                       status = 'active'
                   WHERE ip_port = ?''',
                (datetime.now(), ip_port)
            )
        else:
            self.conn.execute(
                '''UPDATE proxies 
                   SET fail_count = fail_count + 1, 
                       last_check = ?
                   WHERE ip_port = ?''',
                (datetime.now(), ip_port)
            )
        self.conn.commit()
    
    def get_active_proxies(self, limit=100):
        cursor = self.conn.execute(
            '''SELECT ip_port FROM proxies 
               WHERE status = 'active' 
               ORDER BY success_count DESC 
               LIMIT ?''',
            (limit,)
        )
        return [row[0] for row in cursor]
```

## å¸¸ç”¨ç®¡ç†å‘½ä»¤

### 1. é¡¹ç›®ç®¡ç†

**å®Œæ•´é¡¹ç›®åˆå§‹åŒ–ï¼š**
```bash
# ä¸€é”®åˆå§‹åŒ–é¡¹ç›®
cat > setup.sh << 'EOF'
#!/bin/bash

echo "=== ä»£ç†æ± ç®¡ç†ç³»ç»Ÿåˆå§‹åŒ– ==="

# 1. æ£€æŸ¥Pythonç‰ˆæœ¬
python_version=$(python3 --version 2>&1 | grep -oP '\d+\.\d+')
if (( $(echo "$python_version < 3.6" | bc -l) )); then
    echo "é”™è¯¯ï¼šéœ€è¦Python 3.6+ï¼Œå½“å‰ç‰ˆæœ? $python_version"
    exit 1
fi

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
echo "åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
python3 -m venv venv
source venv/bin/activate

# 3. å®‰è£…ä¾èµ–
echo "å®‰è£…ä¾èµ–..."
cd parser_proxy_1
pip install -r requirements.txt
pip install pandas selenium aiohttp

# 4. åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p ../data ../logs

# 5. è®¾ç½®æƒé™
chmod +x ../quick_start.py ../twitter_scraper.py

echo "åˆå§‹åŒ–å®Œæˆï¼"
echo "ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¿€æ´»ç¯å¢ƒï¼š"
echo "source venv/bin/activate"
EOF

chmod +x setup.sh
./setup.sh
```

**é¡¹ç›®çŠ¶æ€æ£€æŸ¥ï¼š**
```bash
# åˆ›å»ºçŠ¶æ€æ£€æŸ¥è„šæœ?
cat > check_status.sh << 'EOF'
#!/bin/bash

echo "=== ä»£ç†æ± ç³»ç»ŸçŠ¶æ€æ£€æŸ?==="

# æ£€æŸ¥ä»£ç†æ–‡ä»?
if [ -f "valid_proxies.txt" ]; then
    proxy_count=$(wc -l < valid_proxies.txt)
    echo "âœ?æœ‰æ•ˆä»£ç†æ–‡ä»¶å­˜åœ¨ï¼ŒåŒ…å?$proxy_count ä¸ªä»£ç?
else
    echo "âœ?æœ‰æ•ˆä»£ç†æ–‡ä»¶ä¸å­˜åœ?
fi

# æ£€æŸ¥æ•°æ®æ–‡ä»?
if [ -f "twitter_data.csv" ]; then
    twitter_count=$(wc -l < twitter_data.csv)
    echo "âœ?Twitteræ•°æ®æ–‡ä»¶å­˜åœ¨ï¼ŒåŒ…å?$twitter_count æ¡è®°å½?
else
    echo "âœ?Twitteræ•°æ®æ–‡ä»¶ä¸å­˜åœ?
fi

# æ£€æŸ¥GitHub Token
if [ -n "$GITHUB_TOKEN" ]; then
    echo "âœ?GitHub Tokenå·²è®¾ç½?
else
    echo "âœ?GitHub Tokenæœªè®¾ç½?
fi

# æ£€æŸ¥ç½‘ç»œè¿æ?
if ping -c 1 google.com &> /dev/null; then
    echo "âœ?ç½‘ç»œè¿æ¥æ­£å¸¸"
else
    echo "âœ?ç½‘ç»œè¿æ¥å¼‚å¸¸"
fi

# æ£€æŸ¥Pythonæ¨¡å—
python3 -c "
import sys
modules = ['requests', 'lxml', 'fake_useragent', 'concurrent.futures']
for module in modules:
    try:
        __import__(module)
        print(f'âœ?{module} æ¨¡å—å¯ç”¨')
    except ImportError:
        print(f'âœ?{module} æ¨¡å—ç¼ºå¤±')
"
EOF

chmod +x check_status.sh
./check_status.sh
```

### 2. å®šæ—¶ä»»åŠ¡ç®¡ç†

**è®¾ç½®ç³»ç»Ÿå®šæ—¶ä»»åŠ¡ï¼?*
```bash
# æ·»åŠ åˆ°crontab
(crontab -l 2>/dev/null; echo "0 */2 * * * cd $(pwd) && python3 quick_start.py >> logs/proxy_update.log 2>&1") | crontab -

# æŸ¥çœ‹å®šæ—¶ä»»åŠ¡
crontab -l

# åˆ é™¤å®šæ—¶ä»»åŠ¡
crontab -r

# åˆ›å»ºsystemdæœåŠ¡
sudo cat > /etc/systemd/system/proxy-pool.service << 'EOF'
[Unit]
Description=Proxy Pool Service
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$(pwd)
Environment=GITHUB_TOKEN=your_token_here
ExecStart=/usr/bin/python3 quick_start.py
Restart=always
RestartSec=3600

[Install]
WantedBy=multi-user.target
EOF

# å¯ç”¨æœåŠ¡
sudo systemctl enable proxy-pool.service
sudo systemctl start proxy-pool.service
sudo systemctl status proxy-pool.service
```

### 3. ç›‘æ§å’Œæ—¥å¿?

**æ—¥å¿—ç®¡ç†ï¼?*
```bash
# åˆ›å»ºæ—¥å¿—æŸ¥çœ‹è„šæœ¬
cat > view_logs.sh << 'EOF'
#!/bin/bash

echo "=== ä»£ç†æ± æ—¥å¿—æŸ¥çœ?==="

echo "1. æœ€è¿‘çš„ä»£ç†æ›´æ–°æ—¥å¿—ï¼?
if [ -f "logs/proxy_update.log" ]; then
    tail -n 20 logs/proxy_update.log
else
    echo "æ— ä»£ç†æ›´æ–°æ—¥å¿?
fi

echo -e "\n2. æœ€è¿‘çš„é”™è¯¯æ—¥å¿—ï¼?
if [ -f "logs/error.log" ]; then
    tail -n 10 logs/error.log
else
    echo "æ— é”™è¯¯æ—¥å¿?
fi

echo -e "\n3. ç³»ç»Ÿèµ„æºä½¿ç”¨ï¼?
echo "CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)%"
echo "å†…å­˜: $(free | grep Mem | awk '{printf("%.1f%%\n", $3/$2 * 100.0)}')"
echo "ç£ç›˜: $(df -h . | tail -1 | awk '{print $5}')"

echo -e "\n4. ç½‘ç»œè¿æ¥çŠ¶æ€ï¼š"
netstat -an | grep :80 | wc -l | xargs echo "HTTPè¿æ¥æ•?"
EOF

chmod +x view_logs.sh
./view_logs.sh
```

**æ€§èƒ½ç›‘æ§ï¼?*
```bash
# åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬
cat > monitor.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import psutil
import time
import json
from datetime import datetime

def monitor_system():
    """ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
    stats = {
        'timestamp': datetime.now().isoformat(),
        'cpu_percent': psutil.cpu_percent(interval=1),
        'memory_percent': psutil.virtual_memory().percent,
        'disk_percent': psutil.disk_usage('.').percent,
        'network_connections': len(psutil.net_connections()),
        'processes': len(psutil.pids())
    }
    
    return stats

def log_stats(stats):
    """è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
    with open('logs/system_stats.json', 'a') as f:
        f.write(json.dumps(stats) + '\n')
    
    print(f"[{stats['timestamp']}] CPU: {stats['cpu_percent']}% "
          f"å†…å­˜: {stats['memory_percent']}% "
          f"ç£ç›˜: {stats['disk_percent']}%")

if __name__ == "__main__":
    print("å¼€å§‹ç³»ç»Ÿç›‘æ?.. (Ctrl+Cåœæ­¢)")
    try:
        while True:
            stats = monitor_system()
            log_stats(stats)
            time.sleep(60)  # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬?
    except KeyboardInterrupt:
        print("\nç›‘æ§åœæ­¢")
EOF

# è¿è¡Œç›‘æ§
python3 monitor.py &
```

### 4. æ•°æ®åˆ†æ

**ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šï¼?*
```bash
# åˆ›å»ºæ•°æ®åˆ†æè„šæœ¬
cat > analyze_data.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import os

def analyze_proxy_data():
    """åˆ†æä»£ç†æ•°æ®"""
    print("=== ä»£ç†æ•°æ®åˆ†æ ===")
    
    if os.path.exists('valid_proxies.txt'):
        with open('valid_proxies.txt', 'r') as f:
            proxies = f.readlines()
        
        print(f"å½“å‰æœ‰æ•ˆä»£ç†æ•°é‡: {len(proxies)}")
        
        # åˆ†æIPæ®µåˆ†å¸?
        ip_segments = {}
        for proxy in proxies:
            ip = proxy.strip().split(':')[0]
            segment = '.'.join(ip.split('.')[:2])
            ip_segments[segment] = ip_segments.get(segment, 0) + 1
        
        print("å‰?0ä¸ªIPæ®µåˆ†å¸?")
        sorted_segments = sorted(ip_segments.items(), key=lambda x: x[1], reverse=True)[:10]
        for segment, count in sorted_segments:
            print(f"  {segment}.x.x: {count}")
    else:
        print("ä»£ç†æ–‡ä»¶ä¸å­˜åœ?)

def analyze_twitter_data():
    """åˆ†æTwitteræ•°æ®"""
    print("\n=== Twitteræ•°æ®åˆ†æ ===")
    
    if os.path.exists('twitter_data.csv'):
        df = pd.read_csv('twitter_data.csv')
        
        print(f"æ€»ç”¨æˆ·æ•°: {len(df)}")
        print(f"æœ‰è”ç³»æ–¹å¼çš„ç”¨æˆ·: {df['has_contact'].sum()}")
        print(f"å¹³å‡ç²‰ä¸æ•? {df['followers'].mean():.0f}")
        print(f"æœ€é«˜ç²‰ä¸æ•°: {df['followers'].max()}")
        
        # ç”Ÿæˆå›¾è¡¨
        plt.figure(figsize=(10, 6))
        plt.hist(df['followers'], bins=50, alpha=0.7)
        plt.title('Twitterç”¨æˆ·ç²‰ä¸æ•°åˆ†å¸?)
        plt.xlabel('ç²‰ä¸æ•?)
        plt.ylabel('ç”¨æˆ·æ•?)
        plt.savefig('twitter_followers_distribution.png')
        print("å·²ç”Ÿæˆç²‰ä¸æ•°åˆ†å¸ƒå›? twitter_followers_distribution.png")
    else:
        print("Twitteræ•°æ®æ–‡ä»¶ä¸å­˜åœ?)

if __name__ == "__main__":
    analyze_proxy_data()
    analyze_twitter_data()
EOF

# å®‰è£…matplotlib
pip install matplotlib

# è¿è¡Œåˆ†æ
python3 analyze_data.py
```

### 5. å¤‡ä»½å’Œæ¢å¤?

**æ•°æ®å¤‡ä»½ï¼?*
```bash
# åˆ›å»ºå¤‡ä»½è„šæœ¬
cat > backup.sh << 'EOF'
#!/bin/bash

BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "åˆ›å»ºå¤‡ä»½åˆ? $BACKUP_DIR"

# å¤‡ä»½ä»£ç†æ–‡ä»¶
cp valid_proxies.txt "$BACKUP_DIR/" 2>/dev/null || echo "ä»£ç†æ–‡ä»¶ä¸å­˜åœ?
cp proxyData.txt "$BACKUP_DIR/" 2>/dev/null || echo "åŸå§‹ä»£ç†æ–‡ä»¶ä¸å­˜åœ?

# å¤‡ä»½Twitteræ•°æ®
cp *.csv "$BACKUP_DIR/" 2>/dev/null || echo "CSVæ–‡ä»¶ä¸å­˜åœ?

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp .env "$BACKUP_DIR/" 2>/dev/null || echo ".envæ–‡ä»¶ä¸å­˜åœ?

# å¤‡ä»½æ—¥å¿—
cp -r logs "$BACKUP_DIR/" 2>/dev/null || echo "æ—¥å¿—ç›®å½•ä¸å­˜åœ?

# å‹ç¼©å¤‡ä»½
tar -czf "${BACKUP_DIR}.tar.gz" "$BACKUP_DIR"
rm -rf "$BACKUP_DIR"

echo "å¤‡ä»½å®Œæˆ: ${BACKUP_DIR}.tar.gz"

# ä¿ç•™æœ€è¿?0ä¸ªå¤‡ä»?
cd backups
ls -t *.tar.gz | tail -n +11 | xargs rm -f
EOF

chmod +x backup.sh
./backup.sh
```

**æ•°æ®æ¢å¤ï¼?*
```bash
# åˆ›å»ºæ¢å¤è„šæœ¬
cat > restore.sh << 'EOF'
#!/bin/bash

if [ $# -eq 0 ]; then
    echo "ç”¨æ³•: ./restore.sh <å¤‡ä»½æ–‡ä»¶å?"
    echo "å¯ç”¨å¤‡ä»½:"
    ls backups/*.tar.gz 2>/dev/null || echo "æ— å¯ç”¨å¤‡ä»?
    exit 1
fi

BACKUP_FILE="$1"

if [ ! -f "$BACKUP_FILE" ]; then
    echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶ $BACKUP_FILE ä¸å­˜åœ?
    exit 1
fi

echo "ä»å¤‡ä»½æ¢å¤? $BACKUP_FILE"

# è§£å‹å¤‡ä»½
tar -xzf "$BACKUP_FILE"

# è·å–è§£å‹ç›®å½•å?
BACKUP_DIR=$(basename "$BACKUP_FILE" .tar.gz)

# æ¢å¤æ–‡ä»¶
cp "$BACKUP_DIR"/*.txt . 2>/dev/null || echo "æ— ä»£ç†æ–‡ä»¶éœ€æ¢å¤"
cp "$BACKUP_DIR"/*.csv . 2>/dev/null || echo "æ— CSVæ–‡ä»¶éœ€æ¢å¤"
cp "$BACKUP_DIR"/.env . 2>/dev/null || echo "æ— é…ç½®æ–‡ä»¶éœ€æ¢å¤"
cp -r "$BACKUP_DIR"/logs . 2>/dev/null || echo "æ— æ—¥å¿—éœ€æ¢å¤"

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf "$BACKUP_DIR"

echo "æ¢å¤å®Œæˆ"
EOF

chmod +x restore.sh
```

## æ€»ç»“

æœ¬ä»£ç†æ± ç®¡ç†ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„ä»£ç†è·å–ã€éªŒè¯ã€å­˜å‚¨å’Œä½¿ç”¨æ–¹æ¡ˆã€‚é€šè¿‡åˆç†é…ç½®å’Œä½¿ç”¨ï¼Œå¯ä»¥æœ‰æ•ˆæ”¯æŒå„ç±»çˆ¬è™«é¡¹ç›®çš„IPä»£ç†éœ€æ±‚ã€‚è®°ä½å§‹ç»ˆéµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾ã€?

### å¿«é€Ÿå¯åŠ¨å‘½ä»¤æ€»ç»“

```bash
# 1. é¡¹ç›®åˆå§‹åŒ?
git clone <your-repo>
cd parser_proxy
./setup.sh

# 2. è·å–å¹¶éªŒè¯ä»£ç?
python3 quick_start.py

# 3. è¿è¡ŒTwitterçˆ¬è™«
python3 twitter_scraper.py

# 4. å¯åŠ¨å®æ—¶ç›‘æ§
python3 monitor.py &

# 5. æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€?
./check_status.sh

# 6. å¤‡ä»½æ•°æ®
./backup.sh
```

å¦‚æœ‰é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®çš„ Issues é¡µé¢æˆ–æäº?Pull Requestã€?

---

æœ€åæ›´æ–°æ—¶é—´ï¼š2024å¹?
